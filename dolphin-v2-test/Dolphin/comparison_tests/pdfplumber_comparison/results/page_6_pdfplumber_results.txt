PDFPlumber Results for: /home/repo-study/Dolphin/demo/page_imgs/page_6.pdf
Processing time: 4.86 seconds
Generated at: 2026-01-30 10:31:07
================================================================================

=== PAGE 1 ===
Deep Residual Learning for Image Recognition
KaimingHe XiangyuZhang ShaoqingRen JianSun
MicrosoftResearch
{kahe,v-xiangz,v-shren,jiansun}@microsoft.com
Abstract 20
Deeper neural networks are more difficult to train. We
presentaresiduallearningframeworktoeasethetraining 10
of networks that are substantially deeper than those used
previously. We explicitly reformulate the layers as learn-
ingresidualfunctionswithreferencetothelayerinputs,in- 00 1 2 iter. 3 (1e4) 4 5 6
steadoflearningunreferencedfunctions. Weprovidecom-
prehensive empirical evidence showing that these residual
networksareeasiertooptimize,andcangainaccuracyfrom
considerablyincreaseddepth. OntheImageNetdatasetwe
evaluateresidualnetswithadepthofupto152layers—8×
deeperthanVGGnets[40]butstillhavinglowercomplex-
ity.Anensembleoftheseresidualnetsachieves3.57%error
ontheImageNettestset.Thisresultwonthe1stplaceonthe
ILSVRC2015classificationtask. Wealsopresentanalysis
onCIFAR-10with100and1000layers.
The depth of representations is of central importance
for many visual recognition tasks. Solely due to our ex-
tremelydeeprepresentations,weobtaina28%relativeim-
provement on the COCO object detection dataset. Deep
residualnetsarefoundationsofoursubmissionstoILSVRC
& COCO 2015 competitions1, where we also won the 1st
placesonthetasksofImageNetdetection,ImageNetlocal-
ization,COCOdetection,andCOCOsegmentation.
1.Introduction
Deep convolutional neural networks [22, 21] have led
to a series of breakthroughs for image classification [21,
49, 39]. Deep networks naturally integrate low/mid/high-
level features [49] and classifiers in an end-to-end multi-
layer fashion, and the “levels” of features can be enriched
by the number of stacked layers (depth). Recent evidence
[40,43]revealsthatnetworkdepthisofcrucialimportance,
and the leading results [40, 43, 12, 16] on the challenging
ImageNetdataset[35]allexploit“verydeep”[40]models,
withadepthofsixteen[40]tothirty[16]. Manyothernon-
trivial visual recognition tasks [7, 11, 6, 32, 27] have also
1http://image-net.org/challenges/LSVRC/2015/ and
http://mscoco.org/dataset/#detections-challenge2015.
)%(
rorre
gniniart
20
10
00 1 2 3 4 5 6 iter. (1e4)
)%(
rorre
tset
56-layer
20-layer
56-layer
20-layer
Figure1.Trainingerror(left)andtesterror(right)onCIFAR-10
with20-layerand56-layer“plain”networks. Thedeepernetwork
hashighertrainingerror,andthustesterror. Similarphenomena
onImageNetispresentedinFig.4.
greatlybenefitedfromverydeepmodels.
Drivenbythesignificanceofdepth,aquestionarises: Is
learning better networks as easy as stacking more layers?
An obstacle to answering this question was the notorious
problemofvanishing/explodinggradients[14,1,8],which
hamper convergence from the beginning. This problem,
however,hasbeenlargelyaddressedbynormalizedinitial-
ization[23,8,36,12]andintermediatenormalizationlayers
[16],whichenablenetworkswithtensoflayerstostartcon-
verging for stochastic gradient descent (SGD) with back-
propagation[22].
When deeper networks are able to start converging, a
degradation problem has been exposed: with the network
depth increasing, accuracy gets saturated (which might be
unsurprising) and then degrades rapidly. Unexpectedly,
such degradation is not caused by overfitting, and adding
morelayerstoasuitablydeepmodelleadstohighertrain-
ingerror,asreportedin[10,41]andthoroughlyverifiedby
ourexperiments. Fig.1showsatypicalexample.
Thedegradation(oftrainingaccuracy)indicatesthatnot
allsystemsaresimilarlyeasytooptimize. Letusconsidera
shallower architecture and its deeper counterpart that adds
morelayersontoit. Thereexistsasolutionbyconstruction
tothedeepermodel: theaddedlayersareidentitymapping,
and the other layers are copied from the learned shallower
model. Theexistenceofthisconstructedsolutionindicates
thatadeepermodelshouldproducenohighertrainingerror
than its shallower counterpart. But experiments show that
ourcurrentsolversonhandareunabletofindsolutionsthat
1770

=== PAGE 2 ===
ImageNet test set, and won the 1st place in the ILSVRC
x
2015 classification competition. The extremely deep rep-
weight layer resentationsalsohaveexcellentgeneralizationperformance
F(x) relu
x on other recognition tasks, and lead us to further win the
weight layer identity 1st places on: ImageNet detection, ImageNet localization,
COCO detection, and COCO segmentation in ILSVRC &
F(x)(cid:1)+(cid:1)x
relu COCO2015competitions. Thisstrongevidenceshowsthat
Figure2.Residuallearning:abuildingblock. theresiduallearningprincipleisgeneric,andweexpectthat
itisapplicableinothervisionandnon-visionproblems.
arecomparablygoodorbetterthantheconstructedsolution
(orunabletodosoinfeasibletime). 2.RelatedWork
In this paper, we address the degradation problem by
introducing a deep residual learning framework. In- Residual Representations. In image recognition, VLAD
stead of hoping each few stacked layers directly fit a [18]isarepresentationthatencodesbytheresidualvectors
desired underlying mapping, we explicitly let these lay- with respect to a dictionary, and Fisher Vector [30] can be
ers fit a residual mapping. Formally, denoting the desired formulated as a probabilistic version [18] of VLAD. Both
underlyingmappingasH(x), weletthestackednonlinear ofthemarepowerfulshallowrepresentationsforimagere-
layersfitanothermappingofF(x):=H(x)−x. Theorig- trieval and classification [4, 47]. For vector quantization,
inalmappingisrecastintoF(x)+x.Wehypothesizethatit encoding residual vectors [17] is shown to be more effec-
iseasiertooptimizetheresidualmappingthantooptimize tivethanencodingoriginalvectors.
the original, unreferenced mapping. To the extreme, if an In low-level vision and computer graphics, for solv-
identity mapping were optimal, it would be easier to push ing Partial Differential Equations (PDEs), the widely used
theresidualtozerothantofitanidentitymappingbyastack Multigrid method [3] reformulates the system as subprob-
ofnonlinearlayers. lems at multiple scales, where each subproblem is respon-
siblefortheresidualsolutionbetweenacoarserandafiner
TheformulationofF(x)+xcanberealizedbyfeedfor-
scale. AnalternativetoMultigridishierarchicalbasispre-
wardneuralnetworkswith“shortcutconnections”(Fig.2).
conditioning[44,45], whichreliesonvariablesthatrepre-
Shortcut connections [2, 33, 48] are those skipping one or
sentresidualvectorsbetweentwoscales. Ithasbeenshown
more layers. In our case, the shortcut connections simply
[3,44,45]thatthesesolversconvergemuchfasterthanstan-
perform identity mapping, and their outputs are added to
dard solvers that are unaware of the residual nature of the
the outputs of the stacked layers (Fig. 2). Identity short-
solutions.Thesemethodssuggestthatagoodreformulation
cut connections add neither extra parameter nor computa-
orpreconditioningcansimplifytheoptimization.
tional complexity. The entire network can still be trained
end-to-endbySGDwithbackpropagation,andcanbeeas-
Shortcut Connections. Practices and theories that lead to
ily implemented using common libraries (e.g., Caffe [19])
shortcutconnections[2,33,48]havebeenstudiedforalong
withoutmodifyingthesolvers.
time. Anearlypracticeoftrainingmulti-layerperceptrons
We present comprehensive experiments on ImageNet (MLPs)istoaddalinearlayerconnectedfromthenetwork
[35] to show the degradation problem and evaluate our input to the output [33, 48]. In [43, 24], a few interme-
method. Weshowthat: 1)Ourextremelydeepresidualnets diate layers are directly connected to auxiliary classifiers
are easy to optimize, but the counterpart “plain” nets (that for addressing vanishing/exploding gradients. The papers
simply stack layers) exhibit higher training error when the of[38,37,31,46]proposemethodsforcenteringlayerre-
depthincreases;2)Ourdeepresidualnetscaneasilyenjoy sponses,gradients,andpropagatederrors,implementedby
accuracygainsfromgreatlyincreaseddepth,producingre- shortcutconnections. In[43],an“inception”layeriscom-
sultssubstantiallybetterthanpreviousnetworks. posedofashortcutbranchandafewdeeperbranches.
SimilarphenomenaarealsoshownontheCIFAR-10set Concurrentwithourwork,“highwaynetworks”[41,42]
[20], suggesting that the optimization difficulties and the present shortcut connections with gating functions [15].
effectsofourmethodarenotjustakintoaparticulardataset. These gates are data-dependent and have parameters, in
Wepresentsuccessfullytrainedmodelsonthisdatasetwith contrast to our identity shortcuts that are parameter-free.
over100layers,andexploremodelswithover1000layers. When a gated shortcut is “closed” (approaching zero), the
On the ImageNet classification dataset [35], we obtain layers in highway networks represent non-residual func-
excellentresultsbyextremelydeepresidualnets. Our152- tions. On the contrary, our formulation always learns
layerresidualnetisthedeepestnetworkeverpresentedon residual functions; our identity shortcuts are never closed,
ImageNet, while still having lower complexity than VGG and all information is always passed through, with addi-
nets [40]. Our ensemble has 3.57% top-5 error on the tional residual functions to be learned. In addition, high-
2771

=== PAGE 3 ===
way networks have not demonstrated accuracy gains with ReLU [29] and the biases are omitted for simplifying no-
extremelyincreaseddepth(e.g.,over100layers). tations. The operation F + x is performed by a shortcut
connection and element-wise addition. We adopt the sec-
3.DeepResidualLearning ondnonlinearityaftertheaddition(i.e.,σ(y),seeFig.2).
TheshortcutconnectionsinEqn.(1)introduceneitherex-
3.1.ResidualLearning
traparameternorcomputationcomplexity. Thisisnotonly
Let us consider H(x) as an underlying mapping to be attractiveinpracticebutalsoimportantinourcomparisons
fit by a few stacked layers (not necessarily the entire net), between plain and residual networks. We can fairly com-
withxdenotingtheinputstothefirstoftheselayers. Ifone pare plain/residual networks that simultaneously have the
hypothesizes that multiple nonlinear layers can asymptoti- same number of parameters, depth, width, and computa-
callyapproximatecomplicatedfunctions2,thenitisequiv- tionalcost(exceptforthenegligibleelement-wiseaddition).
alent to hypothesize that they can asymptotically approxi- The dimensions of x and F must be equal in Eqn.(1).
mate the residual functions, i.e., H(x)−x (assuming that Ifthisisnotthecase(e.g.,whenchangingtheinput/output
the input and output are of the same dimensions). So channels), we can perform a linear projection Ws by the
ratherthanexpectstackedlayerstoapproximateH(x),we shortcutconnectionstomatchthedimensions:
explicitly let these layers approximate a residual function
F(x) := H(x)−x. The original function thus becomes y=F(x,{Wi})+Wsx. (2)
F(x)+x.Althoughbothformsshouldbeabletoasymptot-
icallyapproximatethedesiredfunctions(ashypothesized),
WecanalsouseasquarematrixWsinEqn.(1). Butwewill
showbyexperimentsthattheidentitymappingissufficient
theeaseoflearningmightbedifferent.
foraddressingthedegradationproblemandiseconomical,
This reformulation is motivated by the counterintuitive
phenomenaaboutthedegradationproblem(Fig.1,left). As
andthusWsisonlyusedwhenmatchingdimensions.
The form of the residual function F is flexible. Exper-
we discussed in the introduction, if the added layers can
iments in this paper involve a function F that has two or
beconstructedasidentitymappings,adeepermodelshould
threelayers(Fig.5),whilemorelayersarepossible. Butif
have training error no greater than its shallower counter-
F hasonlyasinglelayer,Eqn.(1)issimilartoalinearlayer:
part. The degradation problem suggests that the solvers
mighthavedifficultiesinapproximatingidentitymappings
y=W1x+x,forwhichwehavenotobservedadvantages.
Wealsonotethatalthoughtheabovenotationsareabout
bymultiplenonlinearlayers. Withtheresiduallearningre-
fully-connectedlayersforsimplicity,theyareapplicableto
formulation, if identity mappings are optimal, the solvers
maysimplydrivetheweightsofthemultiplenonlinearlay-
convolutional layers. The function F(x,{Wi}) can repre-
sentmultipleconvolutionallayers. Theelement-wiseaddi-
erstowardzerotoapproachidentitymappings.
tionisperformedontwofeaturemaps,channelbychannel.
Inrealcases,itisunlikelythatidentitymappingsareop-
timal, but our reformulation may help to precondition the
3.3.NetworkArchitectures
problem. If the optimal function is closer to an identity
mappingthantoazeromapping,itshouldbeeasierforthe Wehavetestedvariousplain/residualnets,andhaveob-
solvertofindtheperturbationswithreferencetoanidentity servedconsistentphenomena. Toprovideinstancesfordis-
mapping,thantolearnthefunctionasanewone. Weshow cussion,wedescribetwomodelsforImageNetasfollows.
byexperiments(Fig.7)thatthelearnedresidualfunctionsin
Plain Network. Our plain baselines (Fig. 3, middle) are
generalhavesmallresponses,suggestingthatidentitymap-
mainlyinspiredbythephilosophyofVGGnets[40](Fig.3,
pingsprovidereasonablepreconditioning.
left). Theconvolutionallayersmostlyhave3×3filtersand
3.2.IdentityMappingbyShortcuts follow two simple design rules: (i) for the same output
feature map size, the layers have the same number of fil-
We adopt residual learning to every few stacked layers. ters; and (ii) if the feature map size is halved, the num-
AbuildingblockisshowninFig.2. Formally,inthispaper ber of filters is doubled so as to preserve the time com-
weconsiderabuildingblockdefinedas: plexity per layer. We perform downsampling directly by
convolutional layers that have a stride of 2. The network
y=F(x,{Wi})+x. (1)
ends with a global average pooling layer and a 1000-way
fully-connected layer with softmax. The total number of
Here x and y are the input and output vectors of the lay-
weightedlayersis34inFig.3(middle).
ers considered. The function F(x,{Wi}) represents the
It isworth noticing thatour model has fewer filters and
residual mapping to be learned. For the example in Fig. 2
lowercomplexitythanVGGnets[40](Fig.3,left).Our34-
that has two layers, F = W2σ(W1x) in which σ denotes
layerbaselinehas3.6billionFLOPs(multiply-adds),which
2Thishypothesis,however,isstillanopenquestion.See[28]. isonly18%ofVGG-19(19.6billionFLOPs).
3772

=== PAGE 4 ===
ResidualNetwork. Basedontheaboveplainnetwork,we
VGG-19 34-layer plain 34-layer residual
insert shortcut connections (Fig. 3, right) which turn the
image image image
s o iz u e t : p 2 u 2 t 4 3x3 conv, 64 network into its counterpart residual version. The identity
shortcuts(Eqn.(1))canbedirectlyusedwhentheinputand
3x3 conv, 64
output are of the same dimensions (solid line shortcuts in
pool, /2
output Fig.3).Whenthedimensionsincrease(dottedlineshortcuts
size: 112 3x3 conv, 128
in Fig. 3), we consider two options: (A) The shortcut still
3x3 conv, 128 7x7 conv, 64, /2 7x7 conv, 64, /2
performs identity mapping, with extra zero entries padded
output pool, /2 pool, /2 pool, /2 forincreasingdimensions. Thisoptionintroducesnoextra
size: 56 3x3 conv, 256 3x3 conv, 64 3x3 conv, 64 parameter;(B)TheprojectionshortcutinEqn.(2)isusedto
3x3 conv, 256 3x3 conv, 64 3x3 conv, 64 match dimensions (done by 1×1 convolutions). For both
3x3 conv, 256 3x3 conv, 64 3x3 conv, 64 options, when the shortcuts go across feature maps of two
3x3 conv, 256 3x3 conv, 64 3x3 conv, 64 sizes,theyareperformedwithastrideof2.
3x3 conv, 64 3x3 conv, 64
3.4.Implementation
3x3 conv, 64 3x3 conv, 64
pool, /2 3x3 conv, 128, /2 3x3 conv, 128, /2 Our implementation for ImageNet follows the practice
output
size: 28 3x3 conv, 512 3x3 conv, 128 3x3 conv, 128 in [21, 40]. The image is resized with its shorter side ran-
3x3 conv, 512 3x3 conv, 128 3x3 conv, 128 domly sampled in [256,480] for scale augmentation [40].
A224×224cropisrandomlysampledfromanimageorits
3x3 conv, 512 3x3 conv, 128 3x3 conv, 128
horizontalflip,withtheper-pixelmeansubtracted[21].The
3x3 conv, 512 3x3 conv, 128 3x3 conv, 128
standardcoloraugmentationin[21]isused.Weadoptbatch
3x3 conv, 128 3x3 conv, 128
normalization (BN) [16] right after each convolution and
3x3 conv, 128 3x3 conv, 128
beforeactivation,following[16]. Weinitializetheweights
3x3 conv, 128 3x3 conv, 128
asin[12]andtrainallplain/residualnetsfromscratch. We
s o iz u e t : p 1 u 4 t pool, /2 3x3 conv, 256, /2 3x3 conv, 256, /2 use SGD with a mini-batch size of 256. The learning rate
3x3 conv, 512 3x3 conv, 256 3x3 conv, 256
startsfrom0.1andisdividedby10whentheerrorplateaus,
3x3 conv, 512 3x3 conv, 256 3x3 conv, 256 andthemodelsaretrainedforupto60×104iterations. We
3x3 conv, 512 3x3 conv, 256 3x3 conv, 256 useaweightdecayof0.0001andamomentumof0.9. We
3x3 conv, 512 3x3 conv, 256 3x3 conv, 256 donotusedropout[13],followingthepracticein[16].
3x3 conv, 256 3x3 conv, 256 Intesting,forcomparisonstudiesweadoptthestandard
3x3 conv, 256 3x3 conv, 256 10-crop testing [21]. For best results, we adopt the fully-
3x3 conv, 256 3x3 conv, 256 convolutional form as in [40, 12], and average the scores
3x3 conv, 256 3x3 conv, 256 at multiple scales (images are resized such that the shorter
3x3 conv, 256 3x3 conv, 256 sideisin{224,256,384,480,640}).
3x3 conv, 256 3x3 conv, 256
4.Experiments
3x3 conv, 256 3x3 conv, 256
o si u z t e p : u 7 t pool, /2 3x3 conv, 512, /2 3x3 conv, 512, /2 4.1.ImageNetClassification
3x3 conv, 512 3x3 conv, 512
WeevaluateourmethodontheImageNet2012classifi-
3x3 conv, 512 3x3 conv, 512
cationdataset[35]thatconsistsof1000classes.Themodels
3x3 conv, 512 3x3 conv, 512
are trained on the 1.28 million training images, and evalu-
3x3 conv, 512 3x3 conv, 512
ated on the 50k validation images. We also obtain a final
3x3 conv, 512 3x3 conv, 512
result on the 100k test images, reported by the test server.
o si u z t e p : u 1 t fc 4096 avg pool avg pool Weevaluatebothtop-1andtop-5errorrates.
fc 4096 fc 1000 fc 1000
Plain Networks. We first evaluate 18-layer and 34-layer
fc 1000
plainnets. The34-layerplainnetisinFig.3(middle). The
Figure3.ExamplenetworkarchitecturesforImageNet. Left: the 18-layerplainnetisofasimilarform. SeeTable1forde-
VGG-19 model [40] (19.6 billion FLOPs) as a reference. Mid- tailedarchitectures.
dle:aplainnetworkwith34parameterlayers(3.6billionFLOPs). TheresultsinTable2showthatthedeeper34-layerplain
Right: a residual network with 34 parameter layers (3.6 billion net has higher validation error than the shallower 18-layer
FLOPs).Thedottedshortcutsincreasedimensions.Table1shows plain net. To reveal the reasons, in Fig. 4 (left) we com-
moredetailsandothervariants.
paretheirtraining/validationerrorsduringthetrainingpro-
cedure. We have observed the degradation problem - the
4773

=== PAGE 5 ===
layername outputsize 18-layer 34-layer 50-layer 101-layer 152-layer
conv1 112×112 7×7,64,stride2
3×3maxpool,stride2
1×1,64 1×1,64 1×1,64
conv2x 56×56 3×3,64 3×3,64
×2 ×3 ⎡ 3×3,64 ⎤×3 ⎡ 3×3,64 ⎤×3 ⎡ 3×3,64 ⎤×3
(cid:2) 3×3,64 (cid:3) (cid:2) 3×3,64 (cid:3)
1×1,256 1×1,256 1×1,256
⎣  ⎣  ⎣ 
1×1,128 1×1,128 1×1,128
3×3,128 3×3,128
conv3x 28×28 ×2 ×4 ⎡ 3×3,128 ⎤×4 ⎡ 3×3,128 ⎤×4 ⎡ 3×3,128 ⎤×8
(cid:2) 3×3,128 (cid:3) (cid:2) 3×3,128 (cid:3)
1×1,512 1×1,512 1×1,512
⎣  ⎣  ⎣ 
1×1,256 1×1,256 1×1,256
3×3,256 3×3,256
conv4x 14×14 ×2 ×6 ⎡ 3×3,256 ⎤×6 ⎡ 3×3,256 ⎤×23 ⎡ 3×3,256 ⎤×36
(cid:2) 3×3,256 (cid:3) (cid:2) 3×3,256 (cid:3)
1×1,1024 1×1,1024 1×1,1024
⎣  ⎣  ⎣ 
1×1,512 1×1,512 1×1,512
3×3,512 3×3,512
conv5x 7×7 ×2 ×3 ⎡ 3×3,512 ⎤×3 ⎡ 3×3,512 ⎤×3 ⎡ 3×3,512 ⎤×3
(cid:2) 3×3,512 (cid:3) (cid:2) 3×3,512 (cid:3)
1×1,2048 1×1,2048 1×1,2048
⎣  ⎣  ⎣ 
1×1 averagepool,1000-dfc,softmax
FLOPs 1.8×109 3.6×109 3.8×109 7.6×109 11.3×109
Table1.ArchitecturesforImageNet. Buildingblocksareshowninbrackets(seealsoFig.5),withthenumbersofblocksstacked. Down-
samplingisperformedbyconv3 1,conv4 1,andconv5 1withastrideof2.
60
50
40
30
20
0 10 20 30 40 50
iter. (1e4)
)%(
rorre
60
50
40
30
plain-18
plain-34
20
0 10 20 30 40 50
iter. (1e4)
)%(
rorre
34-layer
18-layer
18-layer
ResNet-18
ResNet-34 34-layer
Figure4.TrainingonImageNet.Thincurvesdenotetrainingerror,andboldcurvesdenotevalidationerrorofthecentercrops.Left:plain
networksof18and34layers.Right:ResNetsof18and34layers.Inthisplot,theresidualnetworkshavenoextraparametercomparedto
theirplaincounterparts.
plain ResNet reducing of the training error3. The reason for such opti-
18layers 27.94 27.88 mizationdifficultieswillbestudiedinthefuture.
34layers 28.54 25.03
Residual Networks. Next we evaluate 18-layer and 34-
Table2.Top-1error(%,10-croptesting)onImageNetvalidation. layer residual nets (ResNets). The baseline architectures
HeretheResNetshavenoextraparametercomparedtotheirplain arethesameastheaboveplainnets,expectthatashortcut
counterparts.Fig.4showsthetrainingprocedures.
connectionisaddedtoeachpairof3×3filtersasinFig.3
(right). In the first comparison (Table 2 and Fig. 4 right),
weuseidentitymappingforallshortcutsandzero-padding
34-layer plain net has higher training error throughout the forincreasingdimensions(optionA).Sotheyhavenoextra
whole training procedure, even though the solution space parametercomparedtotheplaincounterparts.
of the 18-layer plain network is a subspace of that of the We have three major observations from Table 2 and
34-layerone. Fig. 4. First, the situation is reversed with residual learn-
We argue that this optimization difficulty is unlikely to ing–the34-layerResNetisbetterthanthe18-layerResNet
becausedbyvanishinggradients. Theseplainnetworksare (by2.8%). Moreimportantly,the34-layerResNetexhibits
trained with BN [16], which ensures forward propagated considerablylowertrainingerrorandisgeneralizabletothe
signalstohavenon-zerovariances. Wealsoverifythatthe validationdata. Thisindicatesthatthedegradationproblem
backwardpropagatedgradientsexhibithealthynormswith is well addressed in this setting and we manage to obtain
BN. So neither forward nor backward signals vanish. In accuracygainsfromincreaseddepth.
fact, the 34-layer plain net is still able to achieve compet- Second, compared to its plain counterpart, the 34-layer
itive accuracy (Table 3), suggesting that the solver works
3Wehaveexperimentedwithmoretrainingiterations(3×)andstillob-
tosomeextent. Weconjecturethatthedeepplainnetsmay
servedthedegradationproblem, suggestingthatthisproblemcannotbe
haveexponentiallylowconvergencerates,whichimpactthe feasiblyaddressedbysimplyusingmoreiterations.
5774

=== PAGE 6 ===
64-d 256-d
model top-1err. top-5err.
VGG-16[40] 28.07 9.33 3x3, 64 1x1, 64
relu
GoogLeNet[43] - 9.15 relu 3x3, 64
relu
PReLU-net[12] 24.27 7.38 3x3, 64
1x1, 256
plain-34 28.54 10.02
relu relu
ResNet-34A 25.03 7.76
ResNet-34B 24.52 7.46
Figure 5. A deeper residual function F for ImageNet. Left: a
ResNet-34C 24.19 7.40
buildingblock(on56×56featuremaps)asinFig.3forResNet-
ResNet-50 22.85 6.71
34.Right:a“bottleneck”buildingblockforResNet-50/101/152.
ResNet-101 21.75 6.05
ResNet-152 21.43 5.71
parameter-free, identity shortcuts help with training. Next
Table3.Errorrates(%,10-croptesting)onImageNetvalidation.
weinvestigateprojectionshortcuts(Eqn.(2)). InTable3we
VGG-16isbasedonourtest. ResNet-50/101/152areofoptionB
comparethreeoptions: (A)zero-paddingshortcutsareused
thatonlyusesprojectionsforincreasingdimensions.
forincreasingdimensions,andallshortcutsareparameter-
free (the same as Table 2 and Fig. 4 right); (B) projec-
method top-1err. top-5err.
VGG[40](ILSVRC’14) - 8.43† tionshortcutsareusedforincreasingdimensions,andother
shortcutsareidentity;and(C)allshortcutsareprojections.
GoogLeNet[43](ILSVRC’14) - 7.89
Table3showsthatallthreeoptionsareconsiderablybet-
VGG[40](v5) 24.4 7.1
terthantheplaincounterpart.BisslightlybetterthanA.We
PReLU-net[12] 21.59 5.71
arguethatthisisbecausethezero-paddeddimensionsinA
BN-inception[16] 21.99 5.81
indeedhavenoresiduallearning.Cismarginallybetterthan
ResNet-34B 21.84 5.71
B, and we attribute this to the extra parameters introduced
ResNet-34C 21.53 5.60
by many (thirteen) projection shortcuts. But the small dif-
ResNet-50 20.74 5.25
ferencesamongA/B/Cindicatethatprojectionshortcutsare
ResNet-101 19.87 4.60
notessentialforaddressingthedegradationproblem.Sowe
ResNet-152 19.38 4.49
donotuseoptionCintherestofthispaper,toreducemem-
Table4.Errorrates(%)ofsingle-modelresultsontheImageNet ory/timecomplexityandmodelsizes. Identityshortcutsare
validationset(except†reportedonthetestset).
particularly important for not increasing the complexity of
thebottleneckarchitecturesthatareintroducedbelow.
method top-5err.(test)
VGG[40](ILSVRC’14) 7.32 Deeper Bottleneck Architectures. Next we describe our
GoogLeNet[43](ILSVRC’14) 6.66 deepernetsforImageNet.Becauseofconcernsonthetrain-
VGG[40](v5) 6.8 ing time that we can afford, we modify the building block
PReLU-net[12] 4.94 as a bottleneck design4. For each residual function F, we
BN-inception[16] 4.82 useastackof3layersinsteadof2(Fig.5). Thethreelayers
ResNet(ILSVRC’15) 3.57 are1×1,3×3,and1×1convolutions,wherethe1×1layers
areresponsibleforreducingandthenincreasing(restoring)
Table5.Errorrates(%)ofensembles. Thetop-5errorisonthe
dimensions,leavingthe3×3layerabottleneckwithsmaller
testsetofImageNetandreportedbythetestserver.
input/output dimensions. Fig. 5 shows an example, where
bothdesignshavesimilartimecomplexity.
ResNetreducesthetop-1errorby3.5%(Table2),resulting Theparameter-freeidentityshortcutsareparticularlyim-
fromthesuccessfullyreducedtrainingerror(Fig.4rightvs. portantforthebottleneckarchitectures.Iftheidentityshort-
left). Thiscomparisonverifiestheeffectivenessofresidual cut in Fig. 5 (right) is replaced with projection, one can
learningonextremelydeepsystems. showthatthetimecomplexityandmodelsizearedoubled,
as the shortcut is connected to the two high-dimensional
Last, we also note that the 18-layer plain/residual nets
ends. So identity shortcuts lead to more efficient models
arecomparablyaccurate(Table2),butthe18-layerResNet
forthebottleneckdesigns.
convergesfaster(Fig.4rightvs.left). Whenthenetis“not
50-layer ResNet: We replace each 2-layer block in the
overlydeep”(18layershere),thecurrentSGDsolverisstill
abletofindgoodsolutionstotheplainnet. Inthiscase,the
4Deepernon-bottleneckResNets(e.g.,Fig.5left)alsogainaccuracy
ResNet eases the optimization by providing faster conver-
fromincreaseddepth(asshownonCIFAR-10),butarenotaseconomical
genceattheearlystage. asthebottleneckResNets.Sotheusageofbottleneckdesignsismainlydue
topracticalconsiderations. Wefurthernotethatthedegradationproblem
Identity vs. Projection Shortcuts. We have shown that ofplainnetsisalsowitnessedforthebottleneckdesigns.
6775

=== PAGE 7 ===
34-layernetwiththis3-layerbottleneckblock,resultingin method error(%)
a50-layerResNet(Table1).WeuseoptionBforincreasing Maxout[9] 9.38
dimensions. Thismodelhas3.8billionFLOPs. NIN[25] 8.81
101-layer and 152-layer ResNets: We construct 101- DSN[24] 8.22
layer and 152-layer ResNets by using more 3-layer blocks #layers #params
(Table 1). Remarkably, although the depth is significantly FitNet[34] 19 2.5M 8.39
increased, the 152-layer ResNet (11.3 billion FLOPs) still Highway[41,42] 19 2.3M 7.54(7.72±0.16)
has lower complexity than VGG-16/19 nets (15.3/19.6 bil- Highway[41,42] 32 1.25M 8.80
lionFLOPs). ResNet 20 0.27M 8.75
The 50/101/152-layer ResNets are more accurate than ResNet 32 0.46M 7.51
the34-layeronesbyconsiderablemargins(Table3and4). ResNet 44 0.66M 7.17
We do not observe the degradation problem and thus en- ResNet 56 0.85M 6.97
joysignificantaccuracygainsfromconsiderablyincreased ResNet 110 1.7M 6.43(6.61±0.16)
depth.Thebenefitsofdeptharewitnessedforallevaluation ResNet 1202 19.4M 7.93
metrics(Table3and4).
Table6.ClassificationerrorontheCIFAR-10testset. Allmeth-
Comparisons with State-of-the-art Methods. In Table 4
odsarewithdataaugmentation.ForResNet-110,werunit5times
we compare with the previous best single-model results. andshow“best(mean±std)”asin[42].
Ourbaseline34-layerResNetshaveachievedverycompet-
itive accuracy. Our 152-layer ResNet has a single-model
soourresidualmodelshaveexactlythesamedepth,width,
top-5 validation error of 4.49%. This single-model result
andnumberofparametersastheplaincounterparts.
outperforms all previous ensemble results (Table 5). We
Weuseaweightdecayof0.0001andmomentumof0.9,
combinesixmodelsofdifferentdepthtoformanensemble
andadopttheweightinitializationin[12]andBN[16]but
(only with two 152-layer ones at the time of submitting).
with no dropout. These models are trained with a mini-
This leads to 3.57% top-5 error on the test set (Table 5).
batch size of 128 on two GPUs. We start with a learning
Thisentrywonthe1stplaceinILSVRC2015.
rate of 0.1, divide it by 10 at 32k and 48k iterations, and
4.2.CIFAR-10andAnalysis terminatetrainingat64kiterations,whichisdeterminedon
a45k/5ktrain/valsplit. Wefollowthesimpledataaugmen-
We conducted more studies on the CIFAR-10 dataset
tationin[24]fortraining: 4pixelsarepaddedoneachside,
[20], which consists of 50k training images and 10k test-
and a 32×32 crop is randomly sampled from the padded
ing images in 10 classes. We present experiments trained
image or its horizontal flip. For testing, we only evaluate
onthetrainingsetandevaluatedonthetestset. Ourfocus
thesingleviewoftheoriginal32×32image.
isonthebehaviorsofextremelydeepnetworks,butnoton
Wecomparen = {3,5,7,9},leadingto20,32,44,and
pushingthestate-of-the-artresults, soweintentionallyuse
56-layernetworks. Fig.6(left)showsthebehaviorsofthe
simplearchitecturesasfollows.
plainnets. Thedeepplainnetssufferfromincreaseddepth,
Theplain/residualarchitecturesfollowtheforminFig.3
and exhibit higher training error when going deeper. This
(middle/right). Thenetworkinputsare32×32images,with
phenomenonissimilartothatonImageNet(Fig.4,left)and
theper-pixelmeansubtracted. Thefirstlayeris3×3convo-
onMNIST(see[41]),suggestingthatsuchanoptimization
lutions. Thenweuseastackof6nlayerswith3×3convo-
difficultyisafundamentalproblem.
lutionsonthefeaturemapsofsizes{32,16,8}respectively,
Fig. 6 (middle) shows the behaviors of ResNets. Also
with 2n layers for each feature map size. The numbers of
similar to the ImageNet cases (Fig. 4, right), our ResNets
filtersare{16,32,64}respectively.Thesubsamplingisper-
managetoovercometheoptimizationdifficultyanddemon-
formedbyconvolutionswithastrideof2.Thenetworkends
strateaccuracygainswhenthedepthincreases.
with a global average pooling, a 10-way fully-connected
We further explore n = 18 that leads to a 110-layer
layer,andsoftmax.Therearetotally6n+2stackedweighted
ResNet. In this case, we find that the initial learning rate
layers. Thefollowingtablesummarizesthearchitecture: of 0.1 is slightly too large to start converging5. So we use
0.01towarmupthetraininguntilthetrainingerrorisbelow
outputmapsize 32×32 16×16 8×8
80%(about400iterations),andthengobackto0.1andcon-
#layers 1+2n 2n 2n
tinuetraining. Therestofthelearningscheduleisasdone
#filters 16 32 64
previously. This110-layernetworkconvergeswell(Fig.6,
middle). It has fewer parameters than other deep and thin
When shortcut connections are used, they are connected
to the pairs of 3×3 layers (totally 3n shortcuts). On this 5Withaninitiallearningrateof0.1,itstartsconverging(<90%error)
datasetweuseidentityshortcutsinallcases(i.e.,optionA), afterseveralepochs,butstillreachessimilaraccuracy.
7776

=== PAGE 8 ===
20
10
5
00 1 2 3 4 5 6
iter. (1e4)
)%(
rorre
20
10
plain-20 5
plain-32
plain-44
plain-56
00 1 2 3 4 5 6
iter. (1e4)
)%(
rorre
20
ResNet-20
ResNet-32
ResNet-44
ResNet-56
56-layer ResNet-110
20-layer 20-layer 10
110-layer 5
1
0 4 5 6
iter. (1e4)
)%(
rorre
residual-110
residual-1202
Figure6.TrainingonCIFAR-10. Dashedlinesdenotetrainingerror,andboldlinesdenotetestingerror. Left: plainnetworks. Theerror
ofplain-110ishigherthan60%andnotdisplayed.Middle:ResNets.Right:ResNetswith110and1202layers.
3
2
1
0 20 40 60 80 100
layer index (sorted by magnitude)
dts
3
2
1
0 20 40 60 80 100
layer index (original)
plain-20 plain-56
ResNet-20
ResNet-56
ResNet-110
dts
plain-20 trainingdata 07+12 07++12 plain-56
ResNet-20 testdata VOC07test VOC12test
ResNet-56
ResNet-110 VGG-16 73.2 70.4
ResNet-101 76.4 73.8
Table 7. Object detection mAP (%) on the PASCAL VOC
2007/2012 test sets using baseline Faster R-CNN. See also ap-
pendixforbetterresults.
metric mAP@.5 mAP@[.5,.95]
VGG-16 41.5 21.2
Figure7.Standarddeviations(std)oflayerresponsesonCIFAR- ResNet-101 48.4 27.2
10.Theresponsesaretheoutputsofeach3×3layer,afterBNand
Table 8. Object detection mAP (%) on the COCO validation set
before nonlinearity. Top: the layers are shown in their original
usingbaselineFasterR-CNN.Seealsoappendixforbetterresults.
order.Bottom:theresponsesarerankedindescendingorder.
havesimilartrainingerror. Wearguethatthisisbecauseof
networks such as FitNet [34] and Highway [41] (Table 6), overfitting. The 1202-layer network may be unnecessarily
yetisamongthestate-of-the-artresults(6.43%,Table6). large (19.4M) for this small dataset. Strong regularization
suchasmaxout[9]ordropout[13]isappliedtoobtainthe
Analysis of Layer Responses. Fig. 7 shows the standard
bestresults([9,25,24,34])onthisdataset.Inthispaper,we
deviations (std) of the layer responses. The responses are
use no maxout/dropout and just simply impose regulariza-
the outputs of each 3×3 layer, after BN and before other
tionviadeepandthinarchitecturesbydesign,withoutdis-
nonlinearity (ReLU/addition). For ResNets, this analy-
tracting from the focus on the difficulties of optimization.
sis reveals the response strength of the residual functions.
But combining with stronger regularization may improve
Fig.7showsthatResNetshavegenerallysmallerresponses
results,whichwewillstudyinthefuture.
thantheirplaincounterparts. Theseresultssupportourba-
sic motivation (Sec.3.1) that the residual functions might 4.3.ObjectDetectiononPASCALandMSCOCO
begenerallyclosertozerothanthenon-residualfunctions.
Our method has good generalization performance on
We also notice that the deeper ResNet has smaller magni-
otherrecognitiontasks. Table7and 8showtheobjectde-
tudesofresponses,asevidencedbythecomparisonsamong
tection baseline results on PASCAL VOC 2007 and 2012
ResNet-20, 56, and 110 in Fig. 7. When there are more
[5]andCOCO[26].WeadoptFasterR-CNN[32]asthede-
layers, an individual layer of ResNets tends to modify the
tectionmethod.Hereweareinterestedintheimprovements
signalless.
ofreplacingVGG-16[40]withResNet-101. Thedetection
Exploring Over 1000 layers. We explore an aggressively implementation(seeappendix)ofusingbothmodelsisthe
deep model of over 1000 layers. We set n = 200 that same,sothegainscanonlybeattributedtobetternetworks.
leadstoa1202-layernetwork,whichistrainedasdescribed Mostremarkably,onthechallengingCOCOdatasetweob-
above. Our method shows no optimization difficulty, and taina6.0%increaseinCOCO’sstandardmetric(mAP@[.5,
this 103-layer network is able to achieve training error .95]), which is a 28% relative improvement. This gain is
<0.1% (Fig. 6, right). Its test error is still fairly good solelyduetothelearnedrepresentations.
(7.93%,Table6). Based on deep residual nets, we won the 1st places in
But there are still open problems on such aggressively severaltracksinILSVRC&COCO2015competitions:Im-
deepmodels. Thetestingresultofthis1202-layernetwork ageNetdetection,ImageNetlocalization,COCOdetection,
isworsethanthatofour110-layernetwork, althoughboth andCOCOsegmentation. Thedetailsareintheappendix.
8777

=== PAGE 9 ===
References
[28] G.Montu´far,R.Pascanu,K.Cho,andY.Bengio. Onthenumberof
linearregionsofdeepneuralnetworks.InNIPS,2014.
[1] Y.Bengio,P.Simard,andP.Frasconi.Learninglong-termdependen-
[29] V.NairandG.E.Hinton. Rectifiedlinearunitsimproverestricted
cieswithgradientdescentisdifficult. IEEETransactionsonNeural
boltzmannmachines.InICML,2010.
Networks,5(2):157–166,1994.
[30] F.PerronninandC.Dance.Fisherkernelsonvisualvocabulariesfor
[2] C. M. Bishop. Neural networks for pattern recognition. Oxford
imagecategorization.InCVPR,2007.
universitypress,1995.
[31] T.Raiko,H.Valpola,andY.LeCun. Deeplearningmadeeasierby
[3] W.L.Briggs,S.F.McCormick,etal. AMultigridTutorial. Siam,
lineartransformationsinperceptrons.InAISTATS,2012.
2000.
[32] S.Ren, K.He, R.Girshick, andJ.Sun. FasterR-CNN:Towards
[4] K.Chatfield,V.Lempitsky,A.Vedaldi,andA.Zisserman.Thedevil
real-timeobjectdetectionwithregionproposalnetworks. InNIPS,
isinthedetails: anevaluationofrecentfeatureencodingmethods.
2015.
InBMVC,2011.
[33] B.D.Ripley. Patternrecognitionandneuralnetworks. Cambridge
[5] M.Everingham,L.VanGool,C.K.Williams,J.Winn,andA.Zis-
universitypress,1996.
serman. ThePascalVisualObjectClasses(VOC)Challenge. IJCV,
[34] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and
pages303–338,2010.
Y.Bengio.Fitnets:Hintsforthindeepnets.InICLR,2015.
[6] R.Girshick.FastR-CNN.InICCV,2015.
[35] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,
[7] R.Girshick,J.Donahue,T.Darrell,andJ.Malik. Richfeaturehier-
Z.Huang, A.Karpathy, A.Khosla, M.Bernstein, etal. Imagenet
archiesforaccurateobjectdetectionandsemanticsegmentation. In
largescalevisualrecognitionchallenge.arXiv:1409.0575,2014.
CVPR,2014.
[36] A.M.Saxe,J.L.McClelland,andS.Ganguli. Exactsolutionsto
[8] X.GlorotandY.Bengio. Understandingthedifficultyoftraining
thenonlineardynamicsoflearningindeeplinearneuralnetworks.
deepfeedforwardneuralnetworks.InAISTATS,2010.
arXiv:1312.6120,2013.
[9] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and
[37] N.N.Schraudolph.Acceleratedgradientdescentbyfactor-centering
Y.Bengio.Maxoutnetworks.arXiv:1302.4389,2013.
decomposition.Technicalreport,1998.
[10] K.HeandJ.Sun.Convolutionalneuralnetworksatconstrainedtime
[38] N.N.Schraudolph. Centeringneuralnetworkgradientfactors. In
cost.InCVPR,2015.
Neural Networks: Tricks of the Trade, pages 207–226. Springer,
[11] K.He,X.Zhang,S.Ren,andJ.Sun.Spatialpyramidpoolingindeep 1998.
convolutionalnetworksforvisualrecognition.InECCV,2014.
[39] P.Sermanet,D.Eigen,X.Zhang,M.Mathieu,R.Fergus,andY.Le-
[12] K.He,X.Zhang,S.Ren,andJ.Sun. Delvingdeepintorectifiers: Cun. Overfeat: Integrated recognition, localization and detection
Surpassinghuman-levelperformanceonimagenetclassification. In usingconvolutionalnetworks.InICLR,2014.
ICCV,2015.
[40] K.SimonyanandA.Zisserman. Verydeepconvolutionalnetworks
[13] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and forlarge-scaleimagerecognition.InICLR,2015.
R.R.Salakhutdinov. Improvingneuralnetworksbypreventingco-
[41] R.K.Srivastava,K.Greff,andJ.Schmidhuber. Highwaynetworks.
adaptationoffeaturedetectors.arXiv:1207.0580,2012.
arXiv:1505.00387,2015.
[14] S.Hochreiter. Untersuchungenzudynamischenneuronalennetzen.
[42] R.K.Srivastava,K.Greff,andJ.Schmidhuber. Trainingverydeep
Diplomathesis,TUMunich,1991.
networks.1507.06228,2015.
[15] S.HochreiterandJ.Schmidhuber.Longshort-termmemory.Neural
[43] C.Szegedy,W.Liu,Y.Jia,P.Sermanet,S.Reed,D.Anguelov,D.Er-
computation,9(8):1735–1780,1997.
han,V.Vanhoucke,andA.Rabinovich. Goingdeeperwithconvolu-
[16] S.IoffeandC.Szegedy. Batchnormalization: Acceleratingdeep tions.InCVPR,2015.
networktrainingbyreducinginternalcovariateshift.InICML,2015.
[44] R.Szeliski. Fastsurfaceinterpolationusinghierarchicalbasisfunc-
[17] H.Jegou,M.Douze,andC.Schmid.Productquantizationfornearest tions.TPAMI,1990.
neighborsearch.TPAMI,33,2011.
[45] R.Szeliski. Locallyadaptedhierarchicalbasispreconditioning. In
[18] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and SIGGRAPH,2006.
C.Schmid.Aggregatinglocalimagedescriptorsintocompactcodes.
[46] T.Vatanen,T.Raiko,H.Valpola,andY.LeCun. Pushingstochas-
TPAMI,2012.
ticgradienttowardssecond-ordermethods–backpropagationlearn-
[19] Y.Jia,E.Shelhamer,J.Donahue,S.Karayev,J.Long,R.Girshick, ing with transformations in nonlinearities. In Neural Information
S.Guadarrama,andT.Darrell.Caffe:Convolutionalarchitecturefor Processing,2013.
fastfeatureembedding.arXiv:1408.5093,2014.
[47] A.VedaldiandB.Fulkerson. VLFeat:Anopenandportablelibrary
[20] A.Krizhevsky. Learningmultiplelayersoffeaturesfromtinyim- ofcomputervisionalgorithms,2008.
ages.TechReport,2009.
[48] W.VenablesandB.Ripley. Modernappliedstatisticswiths-plus.
[21] A.Krizhevsky,I.Sutskever,andG.Hinton. Imagenetclassification 1999.
withdeepconvolutionalneuralnetworks.InNIPS,2012.
[49] M.D.ZeilerandR.Fergus.Visualizingandunderstandingconvolu-
[22] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, tionalneuralnetworks.InECCV,2014.
W.Hubbard, andL.D.Jackel. Backpropagationappliedtohand-
writtenzipcoderecognition.Neuralcomputation,1989.
[23] Y.LeCun,L.Bottou,G.B.Orr,andK.-R.Mu¨ller.Efficientbackprop.
InNeuralNetworks:TricksoftheTrade,pages9–50.Springer,1998.
[24] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu. Deeply-
supervisednets.arXiv:1409.5185,2014.
[25] M.Lin,Q.Chen,andS.Yan.Networkinnetwork.arXiv:1312.4400,
2013.
[26] T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ramanan,
P.Dolla´r,andC.L.Zitnick. MicrosoftCOCO:Commonobjectsin
context.InECCV.2014.
[27] J.Long,E.Shelhamer,andT.Darrell. Fullyconvolutionalnetworks
forsemanticsegmentation.InCVPR,2015.
9778

================================================================================
ANALYSIS:
- Total pages processed: 9
- Pages with text: 9
- Total tables detected: 14
- Processing time: 4.86s
